{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/ 2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the drivers and URL\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "url=(\"https://www.naukri.com/\")\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the Function Definition\n",
    "\n",
    "#Let's enter the details in the search column\n",
    "search_field_designation=driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "search_field_location=driver.find_element_by_id(\"qsb-location-sugg\")\n",
    "\n",
    "#Let's enter the job title and location \n",
    "search_field_designation.send_keys(\"Data Analyst\")\n",
    "search_field_location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create empty list for storing the data\n",
    "\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the job tile\n",
    "\n",
    "titles=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in titles:\n",
    "    job_title.append(i.text)\n",
    "job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the job location\n",
    "\n",
    "location=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "for i in location:\n",
    "    job_location.append(i.text)\n",
    "job_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the company name\n",
    "\n",
    "companies=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in companies:\n",
    "    company_name.append(i.text)\n",
    "company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the exoerience required\n",
    "\n",
    "experience=driver.find_elements_by_xpath('//span[@class=\"ellipsis fleft fs12 lh16\"]')\n",
    "for i in experience:\n",
    "    experience_required.append(i.text)\n",
    "experience_required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a DataFrame for our data\n",
    "\n",
    "data_analyst_jobs=pd.DataFrame({})\n",
    "data_analyst_jobs['Job Title']=job_title[0:10]    \n",
    "data_analyst_jobs['Job Location']=job_location[0:10]\n",
    "data_analyst_jobs['Company Name']=company_name[0:10]\n",
    "data_analyst_jobs['Experience Required']=experience_required[0:10]\n",
    "data_analyst_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/ 2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the drivers and URL\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "url=(\"https://www.naukri.com/\")\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the Function Definition\n",
    "\n",
    "#Let's navigate to search column\n",
    "search_field_designation=driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "search_field_location=driver.find_element_by_id(\"qsb-location-sugg\")\n",
    "\n",
    "#Let's enter the job title and location\n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "search_field_location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create empty list for storing the data\n",
    "\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "job_description=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the job tile\n",
    "\n",
    "titles=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in titles:\n",
    "    job_title.append(i.text)\n",
    "job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the job location\n",
    "\n",
    "location=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "for i in location:\n",
    "    job_location.append(i.text)\n",
    "job_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the company name\n",
    "\n",
    "companies=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in companies:\n",
    "    company_name.append(i.text)\n",
    "company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the detail job description\n",
    "\n",
    "from selenium.common.exceptions import  NoSuchElementException\n",
    "urls=[]\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\"):\n",
    "    urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "for url in urls[0:10]:\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        description=driver.find_element_by_xpath(\"//section[@class='job-desc']\").text\n",
    "        job_description.append(description)\n",
    "    except NoSuchElementException :\n",
    "        job_description.append(\"Not Available\")\n",
    "job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a DataFrame for our data\n",
    "\n",
    "data_scientist_jobs=pd.DataFrame({})\n",
    "data_scientist_jobs['Job Title']=job_title[0:10]    \n",
    "data_scientist_jobs['Job Location']=job_location[0:10]\n",
    "data_scientist_jobs['Company Name']=company_name[0:10]\n",
    "data_scientist_jobs['Detailed Job Description']=job_description[0:10]\n",
    "data_scientist_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: In this question you have to scrape data using the filters available on the webpage\n",
    "You have to use the location and salary filter. You have to scrape data for “Data Scientist” designation for first 10 job results. You have to scrape the job-title, job-location, company_name, experience_required. The location filter to be used is “Delhi/NCR” The salary filter to be used is “3-6” lakhs The task will be done as shown in the below steps:\n",
    "\n",
    "First get the webpage https://www.naukri.com/\n",
    "Enter “Data Scientist” in “Skill,Designations,Companies” field.\n",
    "Then click the search button.\n",
    "Then apply the location filter and salary filter by checking the respective boxes\n",
    "Then scrape the data for the first 10 jobs results you get.\n",
    "Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's call the Function Definition\n",
    "\n",
    "def jobs_data_Scientist(url):\n",
    "    driver = webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Let's create empty list for storing the data\n",
    "    ds_job_title=[]\n",
    "    ds_company_name=[]\n",
    "    ds_location=[]\n",
    "    ds_experience=[]\n",
    "    ds_salary=[]\n",
    "    \n",
    "    #Let's navigate to the search column\n",
    "    driver.find_element_by_xpath('//input[@class=\"sugInp\"]').send_keys('Data Scientist')\n",
    "    driver.find_element_by_xpath('//button[@class=\"btn\"]').click()\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Let's create filter for job location\n",
    "    driver.find_element_by_xpath('//span[@title=\"Delhi/NCR\"]').click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Let's create filter for salary\n",
    "    driver.find_element_by_xpath('//span[@title=\"3-6 Lakhs\"]').click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Let's scrape the job tile\n",
    "    job_titles=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "    for i in job_titles:\n",
    "        ds_job_title.append(i.text)\n",
    "    \n",
    "    # Let's scrape the company name\n",
    "    company_name=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "    for i in company_name:\n",
    "        ds_company_name.append(i.text)\n",
    "    \n",
    "    # Let's scrape the company location    \n",
    "    location=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "    for i in location:\n",
    "        ds_location.append(i.text)\n",
    "    \n",
    "        \n",
    "    # Let's scrape the experience\n",
    "    experience=driver.find_elements_by_xpath('//span[@class=\"ellipsis fleft fs12 lh16\"]')\n",
    "    for i in experience:\n",
    "        ds_experience.append(i.text)\n",
    "    \n",
    "    # Let's scrape the salary\n",
    "    salary=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi salary\"]')\n",
    "    for i in salary:\n",
    "        ds_salary.append(i.text)\n",
    "    \n",
    "    # Let's create a DataFrame for our data\n",
    "    ds_jobs=pd.DataFrame({})\n",
    "    ds_jobs['Job Title']=ds_job_title[0:10]\n",
    "    ds_jobs['Company Name']=ds_company_name[0:10]\n",
    "    ds_jobs['Job Location']=ds_location[0:10]\n",
    "    ds_jobs['Experience']=ds_experience[0:10]\n",
    "    return ds_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the Calling Function\n",
    "\n",
    "jobs_data_Scientist('https://www.naukri.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's call the Function Definition\n",
    "\n",
    "def flipkart_sunglasses(url):\n",
    "    \n",
    "    # Let's load the drivers and URL\n",
    "    driver = webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    #Let's navigate to search bar\n",
    "    driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #Let's search for sunglasses product and then click search button\n",
    "    driver.find_element_by_xpath('//input[@title=\"Search for products, brands and more\"]').send_keys('sunglasses')\n",
    "    driver.find_element_by_xpath('//button[@type=\"submit\"]').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Let's create empty list for storing the data\n",
    "    sunglass_brand_name=[]\n",
    "    sunglass_description=[]\n",
    "    sunglass_price=[]\n",
    "    sunglass_discount=[]\n",
    "    \n",
    "    # Let's scrape the data\n",
    "    j=0\n",
    "    while j<3:\n",
    "        time.sleep(3)\n",
    "        brands=driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "        for brand in brands:\n",
    "            sunglass_brand_name.append(brand.text)\n",
    "        \n",
    "        descriptions=driver.find_elements_by_xpath('//a[contains(@class,\"IRpwTa\")]')\n",
    "        for description in descriptions:\n",
    "            sunglass_description.append(description.text)\n",
    "        \n",
    "        prices=driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "        for price in prices:\n",
    "            sunglass_price.append(price.text)\n",
    "        \n",
    "        \n",
    "    # Let's create a DataFrame for our data   \n",
    "    sunglasses_flip=pd.DataFrame({})\n",
    "    sunglasses_flip['Product Brand']=sunglass_brand_name[:100]\n",
    "    sunglasses_flip['Product Description']=sunglass_description[:100]\n",
    "    sunglasses_flip['Price of Product']=sunglass_price[:100]\n",
    "    \n",
    "    return sunglasses_flip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the Calling Function\n",
    "\n",
    "flipkart_sunglasses('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power-\n",
    "adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review_summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's call the Function Definition\n",
    "\n",
    "def iphone_100_review(url):\n",
    "    \n",
    "    # Let's load the drivers and URL\n",
    "    driver = webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Let's create empty list for storing the data\n",
    "    rating=[]\n",
    "    review_summary=[]\n",
    "    full_review=[]\n",
    "    \n",
    "    # Let's scrape the data\n",
    "    j=0\n",
    "    while j<10:\n",
    "    \n",
    "        ratings=driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "        for i in ratings:\n",
    "            rating.append(i.text)\n",
    "        \n",
    "        reviews=driver.find_elements_by_xpath('//p[@class=\"_2-N8zT\"]')\n",
    "        for i in reviews:\n",
    "            review_summary.append(i.text)\n",
    "        \n",
    "        full_reviews=driver.find_elements_by_xpath('//div[@class=\"t-ZTKy\"]')\n",
    "        for i in full_reviews:\n",
    "            full_review.append(i.text)\n",
    "        time.sleep(10)    \n",
    "        \n",
    "        driver.find_element_by_xpath('//a[@class=\"_1LKTO3\"]').click()\n",
    "        time.sleep(10)\n",
    "        j+=1\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # Let's create a DataFrame for our data\n",
    "    iphone_review=pd.DataFrame({})\n",
    "    iphone_review['Rating']=rating\n",
    "    iphone_review['Review Summary']=review_summary\n",
    "    iphone_review['Full Reviews']=full_review\n",
    "    return iphone_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the Calling Function\n",
    "\n",
    "iphone_100_review('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGR3QP11A&marketplace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries.\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting the web driver.\n",
    "driver=webdriver.Chrome(r'C:\\Program Files (x86)\\chromedriver_win32\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.flipkart.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Closing the popup \n",
    "popup_close=driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "popup_close.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding and filling the product and searching using automation.\n",
    "search_product = driver.find_element_by_xpath(\"//input[@class='_3704LK']\")\n",
    "search_product.send_keys('sneakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the list\n",
    "Brand=[]\n",
    "Product_Discription=[]\n",
    "Price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the BRAND,Product_Discription,Price and Discount from page 1 and checking the output\n",
    "Brand_tag1=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "Brand_1=[]\n",
    "for i in Brand_tag1:\n",
    "    brandtag1=i.text\n",
    "    Brand_1.append(brandtag1)\n",
    "\n",
    "Prod_dis1=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "Prod_dis_1=[]\n",
    "for i in Prod_dis1:\n",
    "    Proddis=i.text\n",
    "    Prod_dis_1.append(Proddis)\n",
    "\n",
    "Price1=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "Price_1=[]\n",
    "for i in Price1:\n",
    "    Prices1=i.text\n",
    "    Price_1.append(Prices1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on the next page no.2\n",
    "Next_page2=driver.find_element_by_xpath(\"//a [@href='/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=2']\")\n",
    "Next_page2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the BRAND,Product_Discription,Price and Discount from page 2 \n",
    "Brand_tag2=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "Brand_2=[]\n",
    "for i in Brand_tag2:\n",
    "    brandtag2=i.text\n",
    "    Brand_2.append(brandtag2)\n",
    "\n",
    "Prod_dis2=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "Prod_dis_2=[]\n",
    "for i in Prod_dis2:\n",
    "    Proddis2=i.text\n",
    "    Prod_dis_2.append(Proddis2)\n",
    "\n",
    "Price2=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "Price_2=[]\n",
    "for i in Price2:\n",
    "    Prices2=i.text\n",
    "    Price_2.append(Prices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on the next page no.3\n",
    "Next_page2=driver.find_element_by_xpath(\"//a [@href='/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=3']\")\n",
    "Next_page2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the BRAND,Product_Discription,Price and Discount from page 3 \n",
    "Brand_tag3=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "Brand_3=[]\n",
    "for i in Brand_tag3:\n",
    "    brandtag3=i.text\n",
    "    Brand_3.append(brandtag3)\n",
    "\n",
    "Prod_dis3=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "Prod_dis_3=[]\n",
    "for i in Prod_dis3:\n",
    "    Proddis3=i.text\n",
    "    Prod_dis_3.append(Proddis3)\n",
    "\n",
    "Price3=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "Price_3=[]\n",
    "for i in Price3:\n",
    "    Prices3=i.text\n",
    "    Price_3.append(Prices3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on the next page no.4\n",
    "Next_page2=driver.find_element_by_xpath(\"//a [@href='/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=4']\")\n",
    "Next_page2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the BRAND,Product_Discription,Price and Discount from page 4 \n",
    "Brand_tag4=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "Brand_4=[]\n",
    "for i in Brand_tag4:\n",
    "    brandtag4=i.text\n",
    "    Brand_4.append(brandtag4)\n",
    "\n",
    "Prod_dis4=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "Prod_dis_4=[]\n",
    "for i in Prod_dis4:\n",
    "    Proddis4=i.text\n",
    "    Prod_dis_4.append(Proddis4)\n",
    "\n",
    "Price4=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "Price_4=[]\n",
    "for i in Price4:\n",
    "    Prices4=i.text\n",
    "    Price_4.append(Prices4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand=Brand_1+Brand_2+Brand_3+Brand_4\n",
    "Product_Discription=Prod_dis_1+Prod_dis_2+Prod_dis_3+Prod_dis_4\n",
    "Price=Price_1+Price_2+Price_3+Price_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a data frame.\n",
    "Data_sneakers=pd.DataFrame({})\n",
    "Data_sneakers['Brand'] = Brand[0:100]\n",
    "Data_sneakers['Product_Discription'] = Product_Discription[0:100]\n",
    "Data_sneakers['Price'] = Price[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_sneakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, as shown in the below image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries.\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting the web driver.\n",
    "driver=webdriver.Chrome(r'C:\\Program Files (x86)\\chromedriver_win32\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.myntra.com/shoes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the required filters\n",
    "price_filter=driver.find_element_by_xpath(\"//*[@id='mountRoot']/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label\")\n",
    "price_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_filter=driver.find_element_by_xpath(\"//*[@id='mountRoot']/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label\")\n",
    "color_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetching the BRAND,Product_Discription,Price from page 1 and checking the output\n",
    "Brand_tag1=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "Brand_1=[]\n",
    "for i in Brand_tag1:\n",
    "    brandtag1=i.text\n",
    "    Brand_1.append(brandtag1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prod_dis1=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "Prod_dis_1=[]\n",
    "for i in Prod_dis1:\n",
    "    Proddis1=i.text\n",
    "    Prod_dis_1.append(Proddis1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Price1=driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "Price_1=[]\n",
    "for i in Price1:\n",
    "    Prices1=i.text\n",
    "    Price_1.append(Prices1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_button=driver.find_element_by_xpath(\"//a[@href='https://www.myntra.com/shoes?f=Color%3ABlack_36454f&plaEnabled=false&rf=Price%3A6612.0_13075.0_6612.0%20TO%2013075.0&p=2']\")\n",
    "page_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_tag2=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "Brand_2=[]\n",
    "for i in Brand_tag2:\n",
    "    brandtag2=i.text\n",
    "    Brand_2.append(brandtag2)\n",
    "    \n",
    "Prod_dis2=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "Prod_dis_2=[]\n",
    "for i in Prod_dis2:\n",
    "    Proddis2=i.text\n",
    "    Prod_dis_2.append(Proddis2)\n",
    "\n",
    "Price2=driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "Price_2=[]\n",
    "for i in Price2:\n",
    "    Prices2=i.text\n",
    "    Price_2.append(Prices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_button=driver.find_element_by_xpath(\"//a[@href='https://www.myntra.com/shoes?f=Color%3ABlack_36454f&plaEnabled=false&rf=Price%3A6612.0_13075.0_6612.0%20TO%2013075.0&p=3']\")\n",
    "page_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_tag3=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "Brand_3=[]\n",
    "for i in Brand_tag3:\n",
    "    brandtag3=i.text\n",
    "    Brand_3.append(brandtag3)\n",
    "    \n",
    "Prod_dis3=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "Prod_dis_3=[]\n",
    "for i in Prod_dis3:\n",
    "    Proddis3=i.text\n",
    "    Prod_dis_3.append(Proddis3)\n",
    "\n",
    "Price3=driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "Price_3=[]\n",
    "for i in Price3:\n",
    "    Prices3=i.text\n",
    "    Price_3.append(Prices3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand=Brand_1+Brand_2+Brand_3\n",
    "Product_Discription=Prod_dis_1+Prod_dis_2+Prod_dis_3\n",
    "Price=Price_1+Price_2+Price_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a data frame.\n",
    "shoes_data=pd.DataFrame({})\n",
    "shoes_data['Brand'] = Brand[0:100]\n",
    "shoes_data['Product_Discription'] = Product_Discription[0:100]\n",
    "shoes_data['Price'] = Price[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:\n",
    "\n",
    "    After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries.\n",
    "import pandas as pd\n",
    "import selenium\n",
    "import time\n",
    "\n",
    "#importing Webdriver\n",
    "from selenium import webdriver\n",
    "\n",
    "#importing Exception Handling\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "#importing keys\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting the web driver.\n",
    "driver=webdriver.Chrome(r'C:\\Program Files (x86)\\chromedriver_win32\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting the website.\n",
    "driver.get('https://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entering \"Laptop\" in the search bar\n",
    "lap=driver.find_element_by_xpath(\"//*[@id='twotabsearchtextbox']\")\n",
    "lap.send_keys(\"laptop\")\n",
    "lap.send_keys(Keys.ENTER)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking the i7 radio button\n",
    "filter_button=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in filter_button:\n",
    "    if i.text=='Intel Core i7':\n",
    "        i.click()\n",
    "        break\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking the i9 radio button\n",
    "filter_button=driver.find_elements_by_xpath(\"//a[@class='a-link-normal s-navigation-item']/span\")\n",
    "for i in filter_button:\n",
    "    if i.text=='Intel Core i9':\n",
    "        i.click()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty list\n",
    "Title=[]\n",
    "Ratings=[]\n",
    "Price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping title details\n",
    "title=driver.find_elements_by_xpath(\"//*[@id='search']/div[1]/div/div[1]/div/span[3]/div[2]/div/div/span/div/div/div[2]/div[2]/div/div[1]/h2/a/span\")\n",
    "for i in title:\n",
    "    try:\n",
    "        Title.append(i.text)\n",
    "    except:\n",
    "        Title.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping rating details\n",
    "rating=driver.find_element_by_xpath(\"//*[@id='search']/div[1]/div/div[1]/div/span[3]/div[2]/div[2]/div/span/div/div/div/div/div[2]/div[2]/div/div[2]/div/span[1]/span/a/i[1]\")\n",
    "rating.click()\n",
    "time.sleep(1)\n",
    "ratings=driver.find_elements_by_xpath('//*[@id=\"a-popover-content-6\"]/div/div/div/div[1]/span')\n",
    "for i in ratings:\n",
    "    try:\n",
    "        Ratings.append(i.text)\n",
    "    except:\n",
    "        Ratings.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping price details\n",
    "price=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "for i in price:\n",
    "    try:\n",
    "        Price.append(i.text)\n",
    "    except:\n",
    "        Price.append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Title),len(Ratings),len(Price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=Title[0:10]\n",
    "Ratings=Ratings[0:10]\n",
    "Price=Price[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the data frame\n",
    "Laptop=pd.DataFrame({})\n",
    "Laptop[\"Title\"]=Title\n",
    "Laptop[\"Ratings\"]=Ratings\n",
    "Laptop[\"Price\"]=Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9: Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter\n",
    "“Data Scientist” and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter\n",
    "“Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
